{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## import all necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as sk_preprocessing\n",
    "from sklearn import linear_model as sk_linear_model\n",
    "from sklearn import metrics as sk_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### This code block generates a test dataset based upon a ground truth function and some artifical noise. \n",
    "### Parameters: \n",
    "# number of datapoints \n",
    "n = 20; \n",
    "# minimum and maximum x value \n",
    "x_min = -10; \n",
    "x_max = 10; \n",
    "# standard deviation of the gaussian noise added\n",
    "sigma = 25; \n",
    "# underlying ground truth function\n",
    "def myFunc(x):\n",
    "    y = 3*x*x*x - x + 5\n",
    "    return y\n",
    "\n",
    "### Code: \n",
    "# sample the random points for the dataset between x_min and x_max\n",
    "x_sample = np.random.rand(n, 1)*(x_max - x_min) + x_min\n",
    "# evaluate the ground truth function and add noise to the result \n",
    "y_sample = myFunc(x_sample) + np.random.normal(0, sigma, (n, 1))\n",
    "# plot the dataset \n",
    "plt.scatter(x_sample, y_sample)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "# define the size of the training set in percent. \n",
    "# Do not use too small or too high values as the remaining algorithms require to have a valid training and test dataset. \n",
    "training_perc = 70\n",
    "\n",
    "### Code: \n",
    "# calculate which index splits the data into training and validation set \n",
    "idx_split = int(np.floor(n*training_perc/100))\n",
    "if idx_split < 1: \n",
    "    print('Chosen training set is too small!\\n')\n",
    "if idx_split > (n-1): \n",
    "    print('Chosen training set is too big!\\n')\n",
    "\n",
    "# split the model into training and test sets\n",
    "x_test = x_sample[1:idx_split]\n",
    "x_training = x_sample[(idx_split+1):n]\n",
    "y_test = y_sample[1:idx_split]\n",
    "y_training = y_sample[(idx_split+1):n]\n",
    "# generate equally spaced x values to plot the identified model later\n",
    "x_plot = np.reshape(np.linspace(x_min, x_max, n), (n,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression with Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def ols_regression(PolyDegree, x_test, x_training, x_plot, y_test, y_training):\n",
    "    # Create a preprocessing object for the given polynomial degree configuration\n",
    "    ols_poly = sk_preprocessing.PolynomialFeatures(ols_PolyDegree)\n",
    "    \n",
    "    # Create the design matrices for the test, the training and the plot points \n",
    "    ols_X_test = ols_poly.fit_transform(x_test)\n",
    "    ols_X_training = ols_poly.fit_transform(x_training)\n",
    "    ols_X_plot = ols_poly.fit_transform(x_plot)\n",
    "\n",
    "    # Create linear regression object\n",
    "    ols_regr = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "    \n",
    "    # Train the model based on the training design matrix and the training output data\n",
    "    ols_regr.fit(ols_X_training, y_training)\n",
    "\n",
    "    # Predict the output data for the training, the test and the plot points\n",
    "    ols_y_pred_training = ols_regr.predict(ols_X_training)\n",
    "    ols_y_pred_test = ols_regr.predict(ols_X_test)\n",
    "    ols_y_plot = ols_regr.predict(ols_X_plot)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients: \\n', ols_regr.coef_)\n",
    "    # print the MSE for the training and the test dataset\n",
    "    print(\"Mean squared error on the training dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_training, ols_y_pred_training))\n",
    "    print(\"Mean squared error on the test dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_test, ols_y_pred_test))\n",
    "\n",
    "    # Plot the results of the test dataset and the identified model based on the plot points\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_plot, ols_y_plot, color='blue', linewidth=3)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return ols_y_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression with L2 Regularization ( Ridge Regression )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Code\n",
    "def ridge_regression(PolyDegree, reg_lambda2, x_test, x_training, x_plot, y_test, y_training):\n",
    "    # Create a preprocessing object for the given polynomial degree configuration\n",
    "    ridge_poly = sk_preprocessing.PolynomialFeatures(l2_PolyDegree)\n",
    "    \n",
    "    # Create the design matrices for the test, the training and the plot points \n",
    "    ridge_X_test = ridge_poly.fit_transform(x_test)\n",
    "    ridge_X_training = ridge_poly.fit_transform(x_training)\n",
    "    ridge_X_plot = ridge_poly.fit_transform(x_plot)\n",
    "\n",
    "    # Create linear regression object\n",
    "    ridge_regr = sk_linear_model.Ridge(alpha=reg_lambda2, fit_intercept=False)\n",
    "    \n",
    "    # Train the model based on the training design matrix and the training output data\n",
    "    ridge_regr.fit(ridge_X_training, y_training)\n",
    "\n",
    "    # Predict the output data for the training, the test and the plot points\n",
    "    ridge_y_pred_training = ridge_regr.predict(ridge_X_training)\n",
    "    ridge_y_pred_test = ridge_regr.predict(ridge_X_test)\n",
    "    ridge_y_plot = ridge_regr.predict(ridge_X_plot)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients: \\n', ridge_regr.coef_)\n",
    "    # print the MSE for the training and the test dataset\n",
    "    print(\"Mean squared error on the training dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_training, ridge_y_pred_training))\n",
    "    print(\"Mean squared error on the test dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_test, ridge_y_pred_test))\n",
    "\n",
    "    # Plot the results of the test dataset and the identified model based on the plot points\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_plot, ridge_y_plot, color='blue', linewidth=3)\n",
    "    plt.grid(True)\n",
    "\n",
    "    return ridge_y_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression with L1 regularization ( Lasso Regression )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Code\n",
    "def lasso_regression(PolyDegree, reg_lambda1, x_test, x_training, x_plot, y_test, y_training):\n",
    "    # Create a preprocessing object for the given polynomial degree configuration\n",
    "    lasso_poly = sk_preprocessing.PolynomialFeatures(l1_PolyDegree)\n",
    "    \n",
    "    # Create the design matrices for the test, the training and the plot points \n",
    "    lasso_X_test = lasso_poly.fit_transform(x_test)\n",
    "    lasso_X_training = lasso_poly.fit_transform(x_training)\n",
    "    lasso_X_plot = lasso_poly.fit_transform(x_plot)\n",
    "\n",
    "    # Create linear regression object\n",
    "    lasso_regr = sk_linear_model.Lasso(alpha=reg_lambda1, fit_intercept=False)\n",
    "    \n",
    "    # Train the model based on the training design matrix and the training output data\n",
    "    lasso_regr.fit(lasso_X_training, y_training)\n",
    "\n",
    "    # Predict the output data for the training, the test and the plot points\n",
    "    lasso_y_pred_training = lasso_regr.predict(lasso_X_training)\n",
    "    lasso_y_pred_test = lasso_regr.predict(lasso_X_test)\n",
    "    lasso_y_plot = lasso_regr.predict(lasso_X_plot)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients: \\n', lasso_regr.coef_)\n",
    "    # print the MSE for the training and the test dataset\n",
    "    print(\"Mean squared error on the training dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_training, lasso_y_pred_training))\n",
    "    print(\"Mean squared error on the test dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_test, lasso_y_pred_test))\n",
    "\n",
    "    # Plot the results of the test dataset and the identified model based on the plot points\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_plot, lasso_y_plot, color='blue', linewidth=3)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return lasso_y_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of Polynomial Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "ols_PolyDegree = 5\n",
    "### apply OLS regression\n",
    "ols_y_plot = ols_regression(ols_PolyDegree, x_test, x_training, x_plot, y_test, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "l2_PolyDegree = 5\n",
    "reg_lambda2 = 50\n",
    "### apply ridge regression\n",
    "ridge_y_plot = ridge_regression(l2_PolyDegree, reg_lambda2, x_test, x_training, x_plot, y_test, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "l1_PolyDegree = 5\n",
    "reg_lambda1 = 0.1\n",
    "### apply lasso regression\n",
    "lasso_y_plot = lasso_regression(l1_PolyDegree, reg_lambda1, x_test, x_training, x_plot, y_test, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot and compare data \n",
    "plt.scatter(x_test, y_test, color='black')\n",
    "plt.plot(x_plot, ols_y_plot, color='blue', linewidth=3, label='OLS')\n",
    "plt.plot(x_plot, ridge_y_plot, color='green', linewidth=3, label='Ridge')\n",
    "plt.plot(x_plot, lasso_y_plot, color='red', linewidth=3, label='Lasso')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
